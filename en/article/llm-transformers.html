<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5"><meta name="theme-color" content="#000000"><link rel="manifest" href="/manifest.json"><script src="/sw-register.js"></script><meta name="description" content="Briefly record the conclusions and methods that appear in the official documentation of hugging face. By the way, record some personal thinking and confusion, so that it is convenient to go back and s"><meta property="og:type" content="article"><meta property="og:title" content="Thinkings about the hugging face document"><meta property="og:url" content="https://vccv.cc/en/article/llm-transformers.html"><meta property="og:site_name" content="Yuesir"><meta property="og:description" content="Briefly record the conclusions and methods that appear in the official documentation of hugging face. By the way, record some personal thinking and confusion, so that it is convenient to go back and s"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2024-03-11T11:17:56.000Z"><meta property="article:modified_time" content="2024-03-14T02:17:56.000Z"><meta property="article:author" content="Yuesir"><meta property="article:tag" content="transformers"><meta property="article:tag" content="huggingface"><meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/en/images/favicon.ico"><link rel="icon" type="image/png" href="/en/images/favicon-192x192.png" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon.png"><title>Thinkings about the hugging face document｜Yuesir</title><link rel="stylesheet" href="/en/css/style.css"><link rel="stylesheet" href="/en/css/style-dark.css"><script src="/en/js/switch.js"></script><link rel="stylesheet" href="/en/css/fancybox.css"><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/en/atom.xml" title="Yuesir" type="application/atom+xml">
</head><body class="max-width mx-auto px3 ltr"><div id="header-post"><a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a> <span id="menu"><span id="nav"><ul><li><a href="/en">Home</a></li><li><a href="/en/tags">Tag</a></li><li><a href="/en/search">Search</a></li><li><a href="/en/tools">Tool</a></li><li><a href="/en/about">About</a></li></ul></span><br><span id="actions"><ul><li><a class="icon" aria-label="Next post" href="/en/article/cuda-conda-nlp.html"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class="icon" aria-label="Back to top" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fas fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class="icon" aria-label="Switch theme"><i class="fas fa-lightbulb" aria-hidden="true" onmouseover='$("#i-switch").toggle()' onmouseout='$("#i-switch").toggle()' onclick="switchNightMode()"></i></a></li><li><a class="icon" aria-label="Back to home" href="/en/"><i class="fas fa-home" aria-hidden="true" onmouseover='$("#i-home").toggle()' onmouseout='$("#i-home").toggle()'></i></a></li></ul><span id="i-prev" class="info" style="display:none">Previous post</span> <span id="i-next" class="info" style="display:none">Next post</span> <span id="i-top" class="info" style="display:none">Back to top</span> <span id="i-switch" class="info" style="display:none">Switch theme</span> <span id="i-home" class="info" style="display:none">Back to home</span></span><br><div id="toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Single-GPU-Optimization"><span class="toc-number">1.</span> <span class="toc-text">Single GPU Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16"><span class="toc-number">1.1.</span> <span class="toc-text">FP16</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZERO"><span class="toc-number">1.2.</span> <span class="toc-text">ZERO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-GPU-Optimization"><span class="toc-number">2.</span> <span class="toc-text">Multi-GPU Optimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Confusion"><span class="toc-number">3.</span> <span class="toc-text">Confusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16-1"><span class="toc-number">3.1.</span> <span class="toc-text">FP16</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Small-models-large-batch-sizes"><span class="toc-number">3.1.1.</span> <span class="toc-text">Small models, large batch sizes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Large-model-small-batch-size"><span class="toc-number">3.1.2.</span> <span class="toc-text">Large model, small batch size</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-accumulation"><span class="toc-number">3.2.</span> <span class="toc-text">Gradient accumulation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">4.</span> <span class="toc-text">Reference</span></a></li></ol></div></span></div><div class="content index py4"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header><a href="javascript:void(0);" onclick="switchNightMode()"><h1 class="posttitle" itemprop="name headline">Thinkings about the hugging face document</h1></a><div class="meta"><div class="postdate">Posted: <time datetime="2024-03-11T11:17:56.000Z" itemprop="datePublished">2024-03-11</time> (Updated: <time datetime="2024-03-14T02:17:56.000Z" itemprop="dateModified">2024-03-14</time>)</div><div class="article-category"><i class="fas fa-archive"></i> <a class="category-link" href="/en/categories/thinking/">thinking</a> › <a class="category-link" href="/en/categories/thinking/nlp/">nlp</a> › <a class="category-link" href="/en/categories/thinking/nlp/llm/">llm</a></div><div class="article-tag"><i class="fas fa-tag"></i> <a class="tag-link-link" href="/en/tags/huggingface/" rel="tag">huggingface</a>, <a class="tag-link-link" href="/en/tags/transformers/" rel="tag">transformers</a></div></div></header><div class="content" itemprop="articleBody"><h2 id="Single-GPU-Optimization"><a href="#Single-GPU-Optimization" class="headerlink" title="Single GPU Optimization"></a>Single GPU Optimization</h2><p>Resource: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one">Hugging Face Doc</a></p><table><thead><tr><th>Method/tool</th><th>Improves training speed</th><th>Optimizes memory utilization</th></tr></thead><tbody><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#batch-size-choice">Batch size choice</a></td><td>Yes</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#gradient-accumulation">Gradient accumulation</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#gradient-checkpointing">Gradient checkpointing</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#mixed-precision-training">Mixed precision training</a></td><td>Yes</td><td>(<strong>No</strong>)</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice">Optimizer choice</a></td><td>Yes</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#data-preloading">Data preloading</a></td><td>Yes</td><td>No</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#deepspeed-zero">DeepSpeed Zero</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using-torchcompile">torch.compile</a></td><td>Yes</td><td>No</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using--peft">Parameter-Efficient Fine Tuning (PEFT)</a></td><td>No</td><td>Yes</td></tr></tbody></table><h3 id="FP16"><a href="#FP16" class="headerlink" title="FP16"></a>FP16</h3><blockquote><p>If your model doesn’t work well with mixed precision, for example if it wasn’t pretrained in mixed precision, you may encounter overflow or underflow issues which can cause NaN loss. For these cases, you should use full fp32 precision by explicitly disabling the default fp16 mode.</p></blockquote><h3 id="ZERO"><a href="#ZERO" class="headerlink" title="ZERO"></a>ZERO</h3><p>stage 1 Optimizer state</p><p>stage 2 Optimizer state + gradient</p><p>stage 3 optimizer state + gradient + model parameters (weights)</p><p>The inference stage zero-1 zero-2 does nothing, only zero-3 can be set (no optimizer is needed for the inference stage and no gradient is generated). Also if Transformers&lt;4.28, you need to set <code>synced_gpus=True</code> when generating.</p><blockquote><p>Using multiple GPUs with ZeRO-3 for generation requires synchronizing the GPUs by setting <code>synced_gpus=True</code> in the <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM.generate">generate()</a> method. Otherwise, if one GPU is finished generating before another one, the whole system hangs because the remaining GPUs haven’t received the weight shard from the GPU that finished first.</p><p>For Transformers&gt;=4.28, if <code>synced_gpus</code> is automatically set to <code>True</code> if multiple GPUs are detected during generation.</p></blockquote><p>If offload is configured, you need to choose an optimizer that is adapted to both CPU and GPU, and the commonly used Adam cannot be used.</p><blockquote><p>DeepSpeed and Transformers optimizer and scheduler can be mixed and matched as long as you don’t enable <code>offload_optimizer</code>. When <code>offload_optimizer</code> is enabled, you could use a <u>non-DeepSpeed optimizer</u> (except for LAMB) as long as it has <u>both a CPU and GPU implementation</u>.</p></blockquote><h2 id="Multi-GPU-Optimization"><a href="#Multi-GPU-Optimization" class="headerlink" title="Multi-GPU Optimization"></a>Multi-GPU Optimization</h2><p>Resource: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many">Hugging Face Doc</a></p><p>It’s mostly just a few latitudes of parallelism: Data + Pipeline + Tensor.</p><h2 id="Confusion"><a href="#Confusion" class="headerlink" title="Confusion"></a>Confusion</h2><h3 id="FP16-1"><a href="#FP16-1" class="headerlink" title="FP16"></a>FP16</h3><p>Only speeds up the training process, <strong>not necessarily</strong> reduces the memory footprint (maybe even by a factor of 1.5).</p><blockquote><p>While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. <u>This is because the model is now present on the GPU in both 16-bit and 32-bit precision</u> (1.5x the original model on the GPU). From: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#mixed-precision-training">Hugging Face Doc</a></p><p><u>Note</u>: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a large model and a small batch size, the memory use will be larger.</p></blockquote><h4 id="Small-models-large-batch-sizes"><a href="#Small-models-large-batch-sizes" class="headerlink" title="Small models, large batch sizes"></a>Small models, large batch sizes</h4><ul><li><strong>MEMORY SAVINGS</strong>: For small models, the model parameters and weights take up relatively little memory. When training with large batch sizes, the memory occupied by activations and gradients becomes the main memory usage part. In this case, converting activations and gradients to FP16 can significantly reduce this portion of memory usage, as FP16 occupies half the memory of FP32. Even when considering the retention of FP32-accurate copies of the model weights, the overall memory usage is still reduced because the memory savings of the activations and gradients outweigh the memory overhead of the additional copies of the weights.</li><li><strong>EFFICIENCY AND SAVINGS</strong>: Large batch sizes mean that more data is processed per iteration, which increases the amount of memory required to compute activations and gradients. Since this part now uses FP16, the overall memory requirement is reduced compared to using only FP32.</li></ul><h4 id="Large-model-small-batch-size"><a href="#Large-model-small-batch-size" class="headerlink" title="Large model, small batch size"></a>Large model, small batch size</h4><ul><li><strong>Increased memory usage</strong>: for large models, the model parameters and weights themselves take up a lot of memory. In mixed-precision training, even if the activations and gradients are stored as FP16, a copy of the FP32-precision weights still needs to be retained to ensure the accuracy of the updates. This means that the additional memory overhead (due to the FP32 copy of the weights) is a relatively large part of the total memory footprint relative to the size of the model itself. When the batch size is small, the activations and gradients take up relatively little memory, so the memory saving effect from FP16 is not enough to offset the additional memory usage added by keeping FP32 weight copies.</li><li><strong>Impact of weighted copies</strong>: in this case, the additional memory used by FP32 weighted copies becomes a significant factor because the model itself is large. Even if FP16 is used for activation and gradient, the total memory usage may still increase due to FP32 weighted copies.</li></ul><h3 id="Gradient-accumulation"><a href="#Gradient-accumulation" class="headerlink" title="Gradient accumulation"></a>Gradient accumulation</h3><p>Effective batch = gradient accumulation * actual batch.</p><div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">training_args = TrainingArguments(per_device_train_batch_size=<span class="number">1</span>, gradient_accumulation_steps=<span class="number">4</span>, **default_args)</span><br></pre></td></tr></table></figure></div><p>In the above example, your effective batch size becomes 4.</p><div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>): <span class="comment"># Assume 2 epochs of training</span></span><br><span class="line">    <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):: inputs = torch.tensor(inputs, dtype=torch.float32): <span class="comment"># Suppose train 2 epochs.</span></span><br><span class="line">        inputs = torch.tensor(inputs, dtype=torch.float32)</span><br><span class="line">        labels = torch.tensor(labels, dtype=torch.long)</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels) / accumulation_steps <span class="comment"># Note that this is divided by the number of accumulated steps</span></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># Update model parameters every accumulation_steps</span></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span> <span class="keyword">or</span> (i + <span class="number">1</span>) == <span class="built_in">len</span>(dataloader).</span><br><span class="line">            optimizer.step() <span class="comment"># update parameters</span></span><br><span class="line">            optimizer.zero_grad() <span class="comment"># clear the gradient</span></span><br></pre></td></tr></table></figure></div><p>BP is required each time, but the optimizer updates the parameters only when the cumulative number of gradient steps is reached.</p><blockquote><p>optimizer.step() vs optimizer.zero_grad()</p><p>Normal training flow: zero_grad then step Avoid the gradient computed by previous batch to affect the current optimizer state.</p><p>Gradient accumulation process: step then zero_grad As the name suggests, we want to let the gradient accumulation take effect and then update the parameters at the end.</p></blockquote><p>Prioritize the <code>per_device_train_batch_size</code> setting that guarantees the GPU limit is reached before considering accumulation.</p><blockquote><p>While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can result in a more pronounced training slowdown. Consider the following example. Let’s say, the <code>per_device_train_batch_size=4</code> without gradient accumulation hits the GPU’s limit. If you would like to train with batches of size 64, do not set the <code>per_device_train_batch_size</code> to 1 and <code>gradient_accumulation_steps</code> to 64. Instead, keep <code>per_device_train_batch_size=4</code> and set <code>gradient_accumulation_steps=16</code>. This results in the same effective batch size while making better use of the available GPU resources.</p></blockquote><p>It is not known whether gradient accumulation causes excess memory usage. Or rather I don’t think it does, I think the gradient just changes from one number to another number resulting from multiple accumulations, the data format doesn’t change and the number of bytes taken up doesn’t change. In practice it seems to have an effect, but I can’t find a reason to rationalize it.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one">Hugging Face Doc</a></p><div id="copyright"><style>#easter-egg{border:0;padding:10px 0;position:relative}#easter-egg::before{font-family:"Font Awesome 5 Free";font-weight:900;content:"Page Over \f1b0  Thank You";position:absolute;padding:0 10px;line-height:1px;white-space:nowrap;left:50%;transform:translateX(-50%)}</style><hr id="easter-egg"><blockquote style="padding:0"><p>Author: Yuesir</p><p>Link: <a href="/en/article/llm-transformers.html" target="_blank" title="Thinkings about the hugging face document">https://vccv.cc/en/article/llm-transformers.html</a></p><p>Copyright: All articles except special statements are used <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement, reproduced please indicate the source!</p></blockquote><hr></div></div></article><div class="blog-post-comments"><div id="tcomment"><noscript>Please enable JavaScript to view the comments.</noscript></div></div><div id="footer-post-container"><div id="footer-post"><div id="nav-footer" style="display:none"><ul><li><a href="/en">Home</a></li><li><a href="/en/tags">Tag</a></li><li><a href="/en/search">Search</a></li><li><a href="/en/tools">Tool</a></li><li><a href="/en/about">About</a></li></ul></div><div id="toc-footer" style="display:none"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Single-GPU-Optimization"><span class="toc-number">1.</span> <span class="toc-text">Single GPU Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16"><span class="toc-number">1.1.</span> <span class="toc-text">FP16</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZERO"><span class="toc-number">1.2.</span> <span class="toc-text">ZERO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-GPU-Optimization"><span class="toc-number">2.</span> <span class="toc-text">Multi-GPU Optimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Confusion"><span class="toc-number">3.</span> <span class="toc-text">Confusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16-1"><span class="toc-number">3.1.</span> <span class="toc-text">FP16</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Small-models-large-batch-sizes"><span class="toc-number">3.1.1.</span> <span class="toc-text">Small models, large batch sizes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Large-model-small-batch-size"><span class="toc-number">3.1.2.</span> <span class="toc-text">Large model, small batch size</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-accumulation"><span class="toc-number">3.2.</span> <span class="toc-text">Gradient accumulation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">4.</span> <span class="toc-text">Reference</span></a></li></ol></div><div id="actions-footer"><a id="menu" class="icon" onclick='$("#nav-footer").toggle()'><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a> <a id="toc" class="icon" onclick='$("#toc-footer").toggle()'><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a> <a id="switch" class="icon" onclick="switchNightMode()"><i class="fas fa-lightbulb fa-lg" aria-hidden="true"></i> Theme</a> <a id="top" style="display:none" class="icon" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></div></div></div><footer id="footer"><div class="footer-left">Copyright &copy; 2020-2024 Yuesir <a href="https://github.com/probberechts/hexo-theme-cactus" rel="nofollow" target="_blank">Cactus</a> <a href="/en/atom.xml" target="_blank">RSS</a></div><div class="footer-right"><nav><ul><li><a href="/en">Home</a></li><li><a href="/en/tags">Tag</a></li><li><a href="/en/search">Search</a></li><li><a href="/en/tools">Tool</a></li><li><a href="/en/about">About</a></li></ul></nav></div></footer></div><link rel="preload" href="/en/lib/font-awesome/css/all.min.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css"></noscript><script src="/en/lib/jquery/jquery.min.js"></script><script src="/en/lib/clipboard/clipboard.min.js"></script><script type="text/javascript">$((function(){$(".highlight table").before('<span class="btn-copy tooltipped tooltipped-sw" aria-label="Copy to clipboard"><i class="far fa-clone"></i></span>'),new ClipboardJS(".btn-copy",{text:function(e){return Array.from(e.nextElementSibling.querySelectorAll(".code")).reduce((e,t)=>e+t.innerText+"\n","").replace(/^\s+|\s+$/g,"")}}).on("success",(function(e){e.trigger.setAttribute("aria-label","Copied!"),e.clearSelection()}))}))</script><script src="/en/js/main.js"></script><script src="/en/lib/fancybox/fancybox.umd.js"></script><script src="/en/lib/twikoo/twikoo.all.min.js"></script><script>twikoo.init({envId:"https://twikoo.vccv.cc",el:"#tcomment",lang:"en",onCommentLoaded:function(){for(var t=document.getElementsByClassName("tk-content"),e=0;e<t.length;e++){var n=t[e].getElementsByTagName("img");if(n.length>0)for(var a=0;a<n.length;a++){var o=n[a];if("tk-owo-emotion"!=o.className){var r=document.createElement("a");r.setAttribute("data-fancybox","gallery"),r.setAttribute("data-src",o.getAttribute("src")),r.setAttribute("data-caption","Comment: "+o.getAttribute("alt")),r.appendChild(o.cloneNode(!1)),o.parentNode.insertBefore(r,o.nextSibling),o.remove()}}}}})</script><script src="/lib/april-fool/load.js"></script></body></html>