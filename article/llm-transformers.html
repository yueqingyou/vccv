<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5"><meta name="theme-color" content="#1d1f21"><link rel="manifest" href="/manifest.json"><script src="/sw-register.js"></script><meta name="description" content="简单记录一下hugging face官方文档中出现的结论和方法。顺便记载一些个人思考与困惑之处，方便以后回溯解决。欢迎评论区大佬留言交流。"><meta property="og:type" content="article"><meta property="og:title" content="读hugging face文档有感"><meta property="og:url" content="https://vccv.cc/article/llm-transformers.html"><meta property="og:site_name" content="月青悠"><meta property="og:description" content="简单记录一下hugging face官方文档中出现的结论和方法。顺便记载一些个人思考与困惑之处，方便以后回溯解决。欢迎评论区大佬留言交流。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2024-03-11T11:17:56.000Z"><meta property="article:modified_time" content="2024-03-14T02:17:56.000Z"><meta property="article:author" content="Yuesir"><meta property="article:tag" content="transformers"><meta property="article:tag" content="huggingface"><meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/images/favicon.ico"><link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><title>读hugging face文档有感｜月青悠</title><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/style-dark.css"><link rel="stylesheet" href="/css/fancybox.css"><script src="/js/switch.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="月青悠" type="application/atom+xml">
</head><body class="max-width mx-auto px3 ltr"><div id="header-post"><a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a> <span id="menu"><span id="nav"><ul><li><a href="/">首页</a></li><li><a href="/tags">标签</a></li><li><a href="/search">搜索</a></li><li><a href="/tools">工具</a></li><li><a href="/about">关于</a></li></ul></span><br><span id="actions"><ul><li><a class="icon" aria-label="下一篇" href="/article/conda-linux.html"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class="icon" aria-label="返回顶部" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fas fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class="icon" aria-label="切换主题"><i class="fas fa-lightbulb" aria-hidden="true" onmouseover='$("#i-switch").toggle()' onmouseout='$("#i-switch").toggle()' onclick="switchNightMode()"></i></a></li><li><a class="icon" aria-label="回到首页" href="/"><i class="fas fa-home" aria-hidden="true" onmouseover='$("#i-home").toggle()' onmouseout='$("#i-home").toggle()'></i></a></li></ul><span id="i-prev" class="info" style="display:none">上一篇</span> <span id="i-next" class="info" style="display:none">下一篇</span> <span id="i-top" class="info" style="display:none">返回顶部</span> <span id="i-switch" class="info" style="display:none">切换主题</span> <span id="i-home" class="info" style="display:none">回到首页</span></span><br><div id="toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95GPU%E4%BC%98%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">单GPU优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16"><span class="toc-number">1.1.</span> <span class="toc-text">FP16</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZERO"><span class="toc-number">1.2.</span> <span class="toc-text">ZERO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9AGPU%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">多GPU优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%82%B9"><span class="toc-number">3.</span> <span class="toc-text">混淆点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16-1"><span class="toc-number">3.1.</span> <span class="toc-text">FP16</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-number">3.1.1.</span> <span class="toc-text">小模型，大批量大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B0%8F%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-number">3.1.2.</span> <span class="toc-text">大模型，小批量大小</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1"><span class="toc-number">3.2.</span> <span class="toc-text">梯度累计</span></a></li></ol></li></ol></div></span></div><div class="content index py4"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header><a href="javascript:void(0);" onclick="switchNightMode()"><h1 class="posttitle" itemprop="name headline">读hugging face文档有感</h1></a><div class="meta"><div class="postdate">Posted: <time datetime="2024-03-11T11:17:56.000Z" itemprop="datePublished">2024-03-11</time> (Updated: <time datetime="2024-03-14T02:17:56.000Z" itemprop="dateModified">2024-03-14</time>)</div><div class="article-category"><i class="fas fa-archive"></i> <a class="category-link" href="/categories/thinking/">思考杂记</a> › <a class="category-link" href="/categories/thinking/nlp/">自然语言处理</a> › <a class="category-link" href="/categories/thinking/nlp/llm/">大语言模型</a></div><div class="article-tag"><i class="fas fa-tag"></i> <a class="tag-link-link" href="/tags/huggingface/" rel="tag">huggingface</a>, <a class="tag-link-link" href="/tags/transformers/" rel="tag">transformers</a></div></div></header><div class="content" itemprop="articleBody"><h2 id="单GPU优化"><a href="#单GPU优化" class="headerlink" title="单GPU优化"></a>单GPU优化</h2><p>Resource: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one">Hugging Face Doc</a></p><table><thead><tr><th>Method/tool</th><th>Improves training speed</th><th>Optimizes memory utilization</th></tr></thead><tbody><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#batch-size-choice">Batch size choice</a></td><td>Yes</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#gradient-accumulation">Gradient accumulation</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#gradient-checkpointing">Gradient checkpointing</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#mixed-precision-training">Mixed precision training</a></td><td>Yes</td><td>(<strong>No</strong>)</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice">Optimizer choice</a></td><td>Yes</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#data-preloading">Data preloading</a></td><td>Yes</td><td>No</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#deepspeed-zero">DeepSpeed Zero</a></td><td>No</td><td>Yes</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using-torchcompile">torch.compile</a></td><td>Yes</td><td>No</td></tr><tr><td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using--peft">Parameter-Efficient Fine Tuning (PEFT)</a></td><td>No</td><td>Yes</td></tr></tbody></table><h3 id="FP16"><a href="#FP16" class="headerlink" title="FP16"></a>FP16</h3><blockquote><p>If your model doesn’t work well with mixed precision, for example if it wasn’t pretrained in mixed precision, you may encounter overflow or underflow issues which can cause NaN loss. For these cases, you should use full fp32 precision by explicitly disabling the default fp16 mode.</p></blockquote><h3 id="ZERO"><a href="#ZERO" class="headerlink" title="ZERO"></a>ZERO</h3><p>stage 1 优化器状态</p><p>stage 2 优化器状态 + 梯度</p><p>stage 3 优化器状态 + 梯度 + 模型参数（权重）</p><p>推理阶段 zero-1 zero-2 没什么作用，只能设置zero-3（推理阶段不需要优化器，也不会产生梯度）。另外如果 Transformers&lt;4.28 ，生成时需要设置<code>synced_gpus=True</code>。</p><blockquote><p>Using multiple GPUs with ZeRO-3 for generation requires synchronizing the GPUs by setting <code>synced_gpus=True</code> in the <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM.generate">generate()</a> method. Otherwise, if one GPU is finished generating before another one, the whole system hangs because the remaining GPUs haven’t received the weight shard from the GPU that finished first.</p><p>For Transformers&gt;=4.28, if <code>synced_gpus</code> is automatically set to <code>True</code> if multiple GPUs are detected during generation.</p></blockquote><p>如果配置了offload 则需要选择对CPU和GPU同时适配的优化器，常用的Adam不能使用。</p><blockquote><p>DeepSpeed and Transformers optimizer and scheduler can be mixed and matched as long as you don’t enable <code>offload_optimizer</code>. When <code>offload_optimizer</code> is enabled, you could use a <u>non-DeepSpeed optimizer</u> (except for LAMB) as long as it has <u>both a CPU and GPU implementation</u>.</p></blockquote><h2 id="多GPU优化"><a href="#多GPU优化" class="headerlink" title="多GPU优化"></a>多GPU优化</h2><p>Resource: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many">Hugging Face Doc</a></p><p>主要就是几个纬度的并行：Data + Pipeline + Tensor。</p><h2 id="混淆点"><a href="#混淆点" class="headerlink" title="混淆点"></a>混淆点</h2><h3 id="FP16-1"><a href="#FP16-1" class="headerlink" title="FP16"></a>FP16</h3><p>只能加速训练过程，<strong>不一定</strong>能减少显存占用（甚至可能会占用1.5倍）。</p><blockquote><p>While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. <u>This is because the model is now present on the GPU in both 16-bit and 32-bit precision</u> (1.5x the original model on the GPU). From: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#mixed-precision-training">Hugging Face Doc</a></p><p><u>Note</u>: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a large model and a small batch size, the memory use will be larger.</p></blockquote><h4 id="小模型，大批量大小"><a href="#小模型，大批量大小" class="headerlink" title="小模型，大批量大小"></a>小模型，大批量大小</h4><ul><li><strong>内存节省</strong>：对于小型模型，模型参数和权重占用的内存相对较少。当采用大批量大小进行训练时，激活和梯度占用的内存成为主要的内存使用部分。在这种情况下，将激活和梯度转换为FP16可以显著减少这部分的内存占用，因为FP16占用的内存是FP32的一半。即使考虑到保留FP32精度的模型权重副本，整体内存使用量仍然会减少，因为激活和梯度的内存节省超过了额外权重副本的内存开销。</li><li><strong>效率和节省</strong>：大批量大小意味着每次迭代处理更多的数据，这增加了计算激活和梯度所需的内存量。由于这部分现在使用FP16，所以相比仅使用FP32，总体内存需求降低。</li></ul><h4 id="大模型，小批量大小"><a href="#大模型，小批量大小" class="headerlink" title="大模型，小批量大小"></a>大模型，小批量大小</h4><ul><li><strong>内存使用增加</strong>：对于大型模型，模型参数和权重本身就占用大量内存。在混合精度训练中，即使将激活和梯度存储为FP16，仍需保留一份FP32精度的权重副本以保证更新的准确性。这意味着相对于模型本身大小，额外的内存开销（由于FP32的权重副本）在总内存占用中占比较大。当批量大小较小时，激活和梯度占用的内存相对较少，因此FP16带来的内存节省效果不足以抵消因保持FP32权重副本而增加的额外内存使用。</li><li><strong>权重副本的影响</strong>：在这种情况下，由于模型本身大，所以FP32权重副本占用的额外内存成为了一个重要因素。即使激活和梯度使用了FP16，总内存使用量仍可能因为FP32的权重副本而增加。</li></ul><h3 id="梯度累计"><a href="#梯度累计" class="headerlink" title="梯度累计"></a>梯度累计</h3><p>有效批次 = 梯度累计数 * 实际batch。</p><div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">training_args = TrainingArguments(per_device_train_batch_size=<span class="number">1</span>, gradient_accumulation_steps=<span class="number">4</span>, **default_args)</span><br></pre></td></tr></table></figure></div><p>In the above example, your effective batch size becomes 4.</p><div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># 假设训练2个epoch</span></span><br><span class="line">    <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        inputs = torch.tensor(inputs, dtype=torch.float32)</span><br><span class="line">        labels = torch.tensor(labels, dtype=torch.long)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels) / accumulation_steps  <span class="comment"># 注意这里除以了累计步骤数</span></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 每accumulation_steps次更新一次模型参数</span></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span> <span class="keyword">or</span> (i + <span class="number">1</span>) == <span class="built_in">len</span>(dataloader):</span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br></pre></td></tr></table></figure></div><p>每次都需要BP，但是只有达到累计梯度步数，优化器才更新参数。</p><blockquote><p>optimizer.step() vs optimizer.zero_grad()</p><p>正常训练流程：先zero_grad再step 避免以前的batch计算出的梯度对本次优化器状态造成影响。</p><p>梯度累计流程：先step再zero_grad 顾名思义就是要让梯度累计生效 最后再更新参数。</p></blockquote><p>优先保证达到GPU极限的<code>per_device_train_batch_size</code>设置时再考虑累计。</p><blockquote><p>While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can result in a more pronounced training slowdown. Consider the following example. Let’s say, the <code>per_device_train_batch_size=4</code> without gradient accumulation hits the GPU’s limit. If you would like to train with batches of size 64, do not set the <code>per_device_train_batch_size</code> to 1 and <code>gradient_accumulation_steps</code> to 64. Instead, keep <code>per_device_train_batch_size=4</code> and set <code>gradient_accumulation_steps=16</code>. This results in the same effective batch size while making better use of the available GPU resources.</p></blockquote><p>梯度累计是否会造成多余的显存占用，尚不得而知。或者说我认为不会，我认为梯度只是由一个数字变成了多次累计后得出的另外一个数字，数据格式没有改变，占用的字节数也不会改变。实际操作过程中似乎有影响，但我无法找到原因来对其进行合理解释。</p><div id="copyright"><style>#easter-egg{border:0;padding:10px 0;position:relative}#easter-egg::before{font-family:"Font Awesome 5 Free";font-weight:900;content:"本文结束 \f1b0  感谢阅读";position:absolute;padding:0 10px;line-height:1px;white-space:nowrap;left:50%;transform:translateX(-50%)}</style><hr id="easter-egg"><blockquote style="padding:0"><p>作者：Yuesir</p><p>本文链接：<a href="/article/llm-transformers.html" target="_blank" title="读hugging face文档有感">https://vccv.cc/article/llm-transformers.html</a></p><p>版权声明：本博客所有文章除特别声明外,均采用 <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> 许可协议，转载请注明出处！</p></blockquote><hr></div></div></article><div class="blog-post-comments"><div id="tcomment"><noscript>加载评论需要在浏览器启用 JavaScript 脚本支持</noscript></div></div><div id="footer-post-container"><div id="footer-post"><div id="nav-footer" style="display:none"><ul><li><a href="/">首页</a></li><li><a href="/tags">标签</a></li><li><a href="/search">搜索</a></li><li><a href="/tools">工具</a></li><li><a href="/about">关于</a></li></ul></div><div id="toc-footer" style="display:none"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95GPU%E4%BC%98%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">单GPU优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16"><span class="toc-number">1.1.</span> <span class="toc-text">FP16</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZERO"><span class="toc-number">1.2.</span> <span class="toc-text">ZERO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9AGPU%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">多GPU优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%82%B9"><span class="toc-number">3.</span> <span class="toc-text">混淆点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FP16-1"><span class="toc-number">3.1.</span> <span class="toc-text">FP16</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-number">3.1.1.</span> <span class="toc-text">小模型，大批量大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B0%8F%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-number">3.1.2.</span> <span class="toc-text">大模型，小批量大小</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1"><span class="toc-number">3.2.</span> <span class="toc-text">梯度累计</span></a></li></ol></li></ol></div><div id="actions-footer"><a id="menu" class="icon" onclick='$("#nav-footer").toggle()'><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a> <a id="toc" class="icon" onclick='$("#toc-footer").toggle()'><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a> <a id="switch" class="icon" onclick="switchNightMode()"><i class="fas fa-lightbulb fa-lg" aria-hidden="true"></i> 主题</a> <a id="top" style="display:none" class="icon" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 顶部</a></div></div></div><footer id="footer"><div class="footer-left">Copyright &copy; 2020-2024 Yuesir <a href="https://github.com/probberechts/hexo-theme-cactus" rel="nofollow" target="_blank">Cactus</a> <a href="/atom.xml" target="_blank">RSS</a></div><div class="footer-right"><nav><ul><li><a href="/">首页</a></li><li><a href="/tags">标签</a></li><li><a href="/search">搜索</a></li><li><a href="/tools">工具</a></li><li><a href="/about">关于</a></li></ul></nav></div></footer></div><link rel="preload" href="/lib/font-awesome/css/all.min.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"></noscript><script src="/lib/jquery/jquery.min.js"></script><script src="/js/main.js"></script><script src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript">$((function(){$(".highlight table").before('<span class="btn-copy tooltipped tooltipped-sw" aria-label="复制到粘贴板"><i class="far fa-clone"></i></span>'),new ClipboardJS(".btn-copy",{text:function(e){return Array.from(e.nextElementSibling.querySelectorAll(".code")).reduce((e,t)=>e+t.innerText+"\n","").replace(/^\s+|\s+$/g,"")}}).on("success",(function(e){e.trigger.setAttribute("aria-label","复制成功!"),e.clearSelection()}))}))</script><script src="/lib/fancybox/fancybox.umd.js"></script><script src="/lib/twikoo/twikoo.all.min.js"></script><script>twikoo.init({envId:"https://twikoo.vccv.cc",el:"#tcomment",onCommentLoaded:function(){for(var t=document.getElementsByClassName("tk-content"),e=0;e<t.length;e++){var a=t[e].getElementsByTagName("img");if(a.length>0)for(var n=0;n<a.length;n++){var o=a[n];if("tk-owo-emotion"!=o.className){var r=document.createElement("a");r.setAttribute("data-fancybox","gallery"),r.setAttribute("data-src",o.getAttribute("src")),r.setAttribute("data-caption","评论区："+o.getAttribute("alt")),r.appendChild(o.cloneNode(!1)),o.parentNode.insertBefore(r,o.nextSibling),o.remove()}}}}})</script><script src="/lib/april-fool/load.js"></script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n=c[o],a=n,i=function(){c=c.filter(function(t){return n!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(n)};a.hasAttribute("bg-lazy")?(a.removeAttribute("bg-lazy"),i&&i()):(t=new Image,e=a.getAttribute("data-original"),t.onload=function(){a.src=e,a.removeAttribute("data-original"),i&&i()},a.src!==e&&(t.src=e))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>